# Deployment Guide

## Docker Compose

```bash
docker compose up --build
```

Services:

* `api`: FastAPI backend on port 8000
* `frontend`: React SPA (bound to 4173 internally and proxied via Caddy)
* `postgres`: Application database
* `qdrant`: Vector database
* `ingestion-worker`: Runs ingestion CLI jobs
* `processing-worker`: Runs embedding and matching jobs
* `n8n`: Optional workflow orchestrator

## Kubernetes

The manifests under `infra/kubernetes` provide a starting point for deploying to a managed
cluster. Customize secrets and storage classes before applying:

```bash
kubectl apply -k infra/kubernetes/overlays/dev
```

## CI/CD

GitHub Actions workflows under `.github/workflows` lint, test, build, and publish container
images to a registry. Configure repository secrets `REGISTRY_USERNAME`, `REGISTRY_PASSWORD`, and
`REGISTRY_URL`.

## Bare-metal Ubuntu 22.04 (Contabo VM) Setup

The project can run on a fresh Contabo virtual machine that ships only with
Ubuntu 22.04. The following checklist prepares the host, deploys the stack, and
opens access from your local laptop.

### 1. Prepare the VM

1. SSH into the instance: `ssh ubuntu@<vm-ip>`
2. Update the base system and install build tools:

   ```bash
   sudo apt update && sudo apt upgrade -y
   sudo apt install -y git build-essential curl ca-certificates gnupg lsb-release
   ```

### 2. Install Runtime Prerequisites

EchoGraph expects Docker, Python 3.10+, Node.js 18+, and pnpm for the frontend.

1. **Docker Engine and Compose plugin**

   ```bash
   curl -fsSL https://get.docker.com | sh
   sudo usermod -aG docker "$USER"
   newgrp docker
   ```

   Verify with `docker --version` and `docker compose version`.

2. **Python tooling**

   ```bash
   sudo apt install -y python3-pip python3-venv
   ```

3. **Node.js 18 and pnpm**

   ```bash
   curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
   sudo apt install -y nodejs
   sudo npm install -g pnpm
   ```

### 3. Clone the Repository and Bootstrap

```bash
git clone https://github.com/<your-org>/EchoGraph.git
cd EchoGraph
make bootstrap
```

The bootstrap target configures Python environments, installs backend/frontend
dependencies, and downloads demo documents.

### 4. Configure Environment Variables (Optional)

Create a `.env` file with overrides such as `DATABASE_URL`, `QDRANT_URL`, or
`N8N_WEBHOOK_SECRET` if you are not using the default docker-compose settings.

### 5. Launch the Stack

```bash
docker compose up --build -d
```

Check container status with `docker compose ps` and stream logs via
`docker compose logs -f <service>`.

By default the `frontend` service is only reachable through the bundled
[Caddy](https://caddyserver.com/) reverse proxy. Caddy serves HTTPS on port 443
using its internal certificate authority so every deployment has encryption
from the start.

### 6. Open Firewall Ports

If UFW is enabled, allow inbound traffic for the HTTPS endpoint and, optionally,
HTTP if you want automatic redirects:

```bash
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp
sudo ufw reload
```

Expose any additional ports you require (e.g., 5678 for n8n webhooks).

### 7. Trust the autogenerated certificate

Because browsers cannot validate certificates that come from a private
certificate authority, you must install Caddy's root CA locally the first time
you deploy to a fresh VM. Run the helper script on the VM (inside the
repository directory) to copy the certificate out of the container:

```bash
python3 scripts/export_caddy_root_ca.py --output caddy-root.crt
```

The helper ensures the `caddy` service is running (starting it if necessary)
and copies the root certificate into `caddy-root.crt` on the VM. If you have
disabled the internal certificate authority—by clearing `CADDY_TLS_DIRECTIVE`
to request public certificates, for example—the script exits with guidance
rather than producing an empty file.

Download the certificate to your workstation with `scp` and import it into
your operating system's trust store:

* **Windows**: Run `certmgr.msc`, import the file into “Trusted Root Certification
  Authorities”.
* **macOS**: Open “Keychain Access”, import the file, and mark it as “Always
  Trust”.
* **Linux**: Copy the file into `/usr/local/share/ca-certificates/` and run
  `sudo update-ca-certificates`.

After the certificate is trusted, reload the site and the browser will no longer
flag the connection as insecure even when you access the VM by IP address.

### 8. Access from Your Laptop

Open `https://<vm-ip>` (or `https://<your-domain>` once configured) to use the
reviewer UI. The API is available at `https://<vm-ip>/api`. Caddy also serves
plain HTTP on port 80, so you can fall back to `http://<vm-ip>` if you have not
yet trusted the autogenerated certificate. The `frontend` container itself is
bound to `127.0.0.1:5173`, so browsing directly to `http://<vm-ip>:5173` will
still result in `ERR_CONNECTION_REFUSED` on remote hosts; always approach the
stack through Caddy unless you intentionally reconfigure the compose file.

### 9. Use a public certificate (Optional)

If you control a domain and prefer a publicly trusted certificate, configure
Caddy to request Let's Encrypt certificates instead of the internal CA.

1. Point a DNS `A` (and optionally `AAAA`) record to your VM's IP address.
2. Create or update a `.env` file in the repository root. Include `:443` so IP
   access continues to work alongside the domain:

   ```env
   CADDY_DOMAIN=:443, guidelines.example.com, www.guidelines.example.com
   CADDY_TLS_DIRECTIVE=
   CADDY_ACME_EMAIL=ops@example.com
   ```

3. Restart Caddy so it reloads the environment variables:

   ```bash
   docker compose up -d --force-recreate caddy
   ```

   When `CADDY_DOMAIN` includes public hostnames and `CADDY_TLS_DIRECTIVE` is
   empty, Caddy automatically obtains publicly trusted certificates for the
   listed hostnames.

4. Visit `https://guidelines.example.com` to confirm the browser no longer
   prompts for certificate trust.

### 10. Synchronize Keycloak database credentials

Self-hosted deployments that enable Keycloak for authentication must ensure the
PostgreSQL role and database match the credentials configured for the Keycloak
container. Run the helper script from the repository root after Docker services
are up:

```bash
python3 scripts/sync_keycloak_credentials.py \
  --postgres-url postgresql://postgres:postgres@localhost:5432/postgres \
  --keycloak-username keycloak \
  --keycloak-password "$KEYCLOAK_DB_PASSWORD" \
  --keycloak-database keycloak
```

The script creates the role or updates its password as needed and assigns
ownership of the Keycloak database. Using parameterised SQL via `psycopg`
prevents the "syntax error at or near" failures that could arise when passwords
contain special characters.
